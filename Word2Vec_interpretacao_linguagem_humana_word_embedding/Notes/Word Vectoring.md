# ðŸ”µ One Hot Encoding
Differently from the **bag of words** and **TF-IDF** models seen so far, that represents texts as a single vetor of words, **one hot encoding** represents each word in a sentence by a different vector. 

Each vector consists of all zeros, besides a single number 1 (one hot) that represents the respective word. 
- This implies that the vectors in this encoding have size $N$ equal to the number of unique words in the vocabulary.

A complete sentence/text is then represented by a matrix, with all vectors that compose each singular word.

![[Pasted image 20240819140928.png]]

### Problem
The one hot encoding does not see much use because in a significant database vectors get too large, and this factor start presenting problems with respect to data and computational resources. 

Another problem is that the vector's size increase with the amount of training data, which is bad trade-off. 


# ðŸ”µ Word2Vec
This technique is part of class named word embedding. Word2Vec tries to represent words in $N$ space in which their position in this plane represent their meaning, clustering words that have similar meaning.
- This process is very similar to clustering.

![[Pasted image 20240819151623.png]]

With this method we can represent each word as vector of fewer dimensions, which is way less than the usual NLP techniques.
- The vector in this representation is called **dense**, since every position has a meaningful value (it doesn't waste space)


## ðŸ”· Training - Embedding
The amount of numbers used for each word (size of the vector) represents the amount of information each word will carry and how that word can be used in different contexts. 
- For example, *good* can be used in a good sense or in a (bad) sarcastic sense, a 2D vector would capture these 2 information about the word *good*.

Word2vec models are trained with neural networks, using one of the two methods listed bellow. The idea is that the neural network will adjust the weights (values) in each word vector so that it is better at making predictions of future words from a given input.

Its important to notice that word similarity takes a lot of information in context. For example, this method is capable of grouping geographically close countries closer than other countries, inside the same group (country).

ðŸ›‘ This process is called **word embedding** 

### ðŸ”¹ 1. CBOW
Continuous Bag of Words - This technique refers to the process of giving a context, to predict the word that best fits this context.

![[Pasted image 20240819152019.png]]

### ðŸ”¹ 2. Skip Gram
With this method we try to predict the context, from a word. 
![[Pasted image 20240819152149.png]]

- With a smaller amount of data, skip gram is able to generate vectors that contextualize the sentences better
- While CBOW has a faster training process.

## ðŸ”· Hyperspace
Words are distributed over a hyperspace of dimension $N$ and they're also clustered based on context and meaning. This means that we can apply mathematical operations to these words (vectors) to generate another words (vectors) that are related to the operation.

For example, 
$$
\text{nuvens} + \text{estrela} - \text{nuvem} = \text{estrelas}
$$
The idea is that using words that have similar context and applying operations, we can generate modified versions of words from similar groups.

### ðŸŸ¢ In Python
Given that you already have a word2vec model trained, this process can be achieved by getting the most similar words from the operation:

```python
modelo.most_similar(positive=["nuvens", "estrela"], negative=["nuvem"])
```


### ðŸ”· Bias
Words generated by a word2vec algorithm will follow usual human conversations biases and reproduce bad behaviors like racism, prejudice, etc.

For example, we can generate "professora" from the following operation, since woman are commonly associated with teaching, while we cannot generate "medica", since women in healthcare are more associated with nursing than doctors. 

```python
modelo.most_similar(positive=["mÃ©dico", "mulher"], negative=["homem"])
>> "enfermeira"

modelo.most_similar(positive=["professor", "mulher"], negative=["homem"])
>> "professora"
```


## ðŸ”· Vector Combination
If you want to analyse a sentence/text you'll need a way to combine your different vectors in a way that maintains meaning. Then, you'll input the resulting vector to a ML model. 

There are a series of ways to combine words/vectors for this purpose. You must find which best serves your problem.